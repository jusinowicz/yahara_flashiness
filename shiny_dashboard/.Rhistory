group_1
group_2
df <- merge(group_1, group_2)
df
tm1
df <- merge(group_1, tm1)
df
tm2 = merge(t3,tm1)
tm2
t3
t3 = data.frame(t3)
tm2 = merge(t3,tm1)
tm2
lake_models2 = lake_models
model_form [[1]] = "level ~ 
    s(level1,bs=\"cr\",k=6)+s(level2,bs=\"cr\",k=6)+s(level3,bs=\"cr\",k=6)+
    s(rn,bs=\"cr\",k=6)+s(rn1,bs=\"cr\",k=6)+
    s(rn2,bs=\"cr\",k=6)+s(rn3,bs=\"cr\",k=6)+
    te(rn,rn1,k=20)+te(rn1,rn2,k=20)+te(rn2,rn3,k=20)"
model_form [[2]] = "level ~ 
    s(level1,bs=\"cr\",k=6)+s(level2,bs=\"cr\",k=6)+s(level3,bs=\"cr\",k=6)+
    s(rn,bs=\"cr\",k=6)+s(rn1,bs=\"cr\",k=6)+
    s(rn2,bs=\"cr\",k=6)+s(rn3,bs=\"cr\",k=6)+
    te(rn,rn1,k=20)+te(rn1,rn2,k=20)+te(rn2,rn3,k=20)"
n_lakes = length(model_form)
    #Store fitted models
    lake_models = vector("list", n_lakes)
    #Loop over lakes and fit models. Assuming that best-fit models have 
    #already been determined by AIC and GCV. 
    for(n in 1:n_lakes){ 
        # Use the residuals from the GARCH model so that the trends in variance are
        # removed. Note, this version only fits the GARCH part because the AR will be
        # fit by the GAM: 
        lake_gfit1=garchFit( ~arma(0,0)+garch(1,1),
                     data=na.exclude(lake_data[[n]][,2,drop=F]), trace=F)
        # New lake-level time series based on residuals
        lake_new=as.matrix(lake_gfit1@residuals)
        # New time series after removing NAs in the rain
        rn_new=as.matrix(lake_data[[n]]$rn[!is.na(lake_data[[n]][,"rn",drop=T])])
        lake_new = as.matrix(lake_new[!is.na(lake_data[[n]][,"rn",drop=T])])
        colnames(rn_new) = "rn"
        colnames(lake_new) = "level"
        #Combine all of the data, add the lagged data, and turn into ts
        lake_r = make.flashiness.object( lake_new , rn_new, lags)
        # The best-fit GAMs were determined in Usinowicz et al. 2016. 
        # Those are what are fit here.
        # Use bam() (instead of gam()) from mgcv because it is designed for 
        # large data sets.
        lake_models[[n]] = bam ( as.formula((model_form [[n]] )), data=lake_r)
    }
AIC(lake_models[[n]], lake_models2[[n]])
AIC(lake_models[[1]], lake_models2[[1]])
AIC(lake_models[[2]], lake_models2[[2]])
summary(lake_models[[1]])
summary(lake_models[[2]])
n
head(lake_data[[1]])
head(lake_data[[2]])
head(lake_table[[1]])
head(lake_table[[2]])
 #Preallocate the important historical data tables 
  lake_table = vector("list", n_lakes)
  daily_precip = vector("list", n_lakes)
  #Final data set
  lake_data = vector("list", n_lakes)
  #Load the historic data sets
  lake_table[[1]] = read.csv(file = "./../data/men_hist.csv")
  lake_table[[1]][,"time"] = ymd(lake_table[[1]][,"time"])
  lake_table[[2]] = read.csv(file = "./../data/mon_hist.csv")
  lake_table[[2]][,"time"] = ymd(lake_table[[2]][,"time"])
  daily_precip[[1]] = read.csv(file = "./../data/rain_hist.csv")
  daily_precip[[1]][,"time"] = ymd(daily_precip[[1]][,"time"])
  daily_precip[[2]] = daily_precip[[1]]
  #Final processing steps of the raw data which joins lake
  #and precip and truncates to desired start date. 
  for (n in 1: n_lakes){
    #Join the lake and rain data to match up dates
    lake_data[[n]] = lake_table[[n]] %>%
          inner_join(daily_precip[[n]], by = "time" )
    #Truncate the data set so that we only have from real_start
    #onwards. 
    lake_data[[n]] = lake_data[[n]][lake_data[[n]][,"time"] 
                      >= real_start[[n]], ]
    #Do some processing to remove ice-on days (approximately). This 
    #function automatically removes winter days and converts data 
    #table to a timeseries (ts) object 
    lake.tmp = remove.days(lake_data[[n]]$level, year(real_start[[n]] ) )
    colnames(lake.tmp) = "level"
    rn.tmp = remove.days(lake_data[[n]]$rn, year(real_start[[n]] ) )
    colnames(rn.tmp) = "rn"
    #This final step creates the full data object, with lags of 
    #lake level for autocorrelation and lags of rain for delayed
    #rain input. 
    lake_data[[n]] = make.flashiness.object(lake.tmp, rn.tmp, lags)
  }
head(lake_data[[1]])
head(lake_data[[2]])
 #Preallocate the important historical data tables 
  lake_table = vector("list", n_lakes)
  daily_precip = vector("list", n_lakes)
  #Final data set
  lake_data = vector("list", n_lakes)
  #Load the historic data sets
  lake_table[[1]] = read.csv(file = "./../data/men_hist.csv")
  lake_table[[1]][,"time"] = ymd(lake_table[[1]][,"time"])
  lake_table[[2]] = read.csv(file = "./../data/mon_hist.csv")
  lake_table[[2]][,"time"] = ymd(lake_table[[2]][,"time"])
  daily_precip[[1]] = read.csv(file = "./../data/rain_hist.csv")
  daily_precip[[1]][,"time"] = ymd(daily_precip[[1]][,"time"])
  daily_precip[[2]] = daily_precip[[1]]
n
n=1
#Join the lake and rain data to match up dates
    lake_data[[n]] = lake_table[[n]] %>%
          inner_join(daily_precip[[n]], by = "time" )
head(lake_data[[1]])
 lake_data[[n]] = lake_data[[n]][lake_data[[n]][,"time"] 
                      >= real_start[[n]], ]
head(lake_data[[1]])
 lake.tmp = remove.days(lake_data[[n]]$level, year(real_start[[n]] ) )
    colnames(lake.tmp) = "level"
    rn.tmp = remove.days(lake_data[[n]]$rn, year(real_start[[n]] ) )
    colnames(rn.tmp) = "rn"
head(lake.tmp)
head(rn.tmp)
    lake_data[[n]] = make.flashiness.object(lake.tmp, rn.tmp, lags)
head(lake_data[[n]])
n=2
 #Join the lake and rain data to match up dates
    lake_data[[n]] = lake_table[[n]] %>%
          inner_join(daily_precip[[n]], by = "time" )
    #Truncate the data set so that we only have from real_start
    #onwards. 
    lake_data[[n]] = lake_data[[n]][lake_data[[n]][,"time"] 
                      >= real_start[[n]], ]
head(lake_data[[n]])
lake_table[[1]][21397:21400,]
lake_table[[2]][21397:21400,]
lake_table[[1]] = read.csv(file = "./../../flashiness2016 - Copy/data/men_hist.csv
)
)
)
lake_table[[1]] = read.csv(file = "./../../flashiness2016 - Copy/data/men_hist.csv")
lake_table[[2]][21397:21400,]
lake_table[[2]] = read.csv(file = "./../../flashiness2016 - Copy/data/mon_hist.csv")
lake_table[[2]][21397:21400,]
dim(lake_Table[[2]])
dim(lake_table[[2]])
dim(lake_table[[1]])
lake_table[[1]][21397:21400,]
lake_table[[1]][19800,]
library(shiny)
library(tidyverse)
library(lubridate)
#Data processing
library(nasapower) #API for NASA data, for precipitation
library(openmeteo)
#Stats
library(mgcv)
library(fGarch) #For GARCH models
#misc data processing and stats
source("./../functions/flash_functions.R")
##############################################################
#Global variables
##############################################################
#USGS site keys. Currently includes Mendota and Monona
site_keys = c("05428000", "05429000")
n_lakes = length(site_keys)
#The base urls used by the USGS for lake data
url_base = c("https://waterservices.usgs.gov/nwis/iv/?format=rdb&sites=")
#Parameters from the NASA POWER data collection
nasa_pars = c("PRECTOTCORR")
#Where the historical data should start
#How long of a data set? Currently 10 years
#real_start = list( ymd("1980-1-1"), ymd("1980-1-1")) #Too big to save
real_start = list( ymd("2013-1-1"), ymd("2013-1-1"))
#Where the lake stage and rain data live: 
lake_data = vector("list", n_lakes)
#Max lags in rain and lake-level data
lags = 10
  #Preallocate the important historical data tables 
  lake_table = vector("list", n_lakes)
  daily_precip = vector("list", n_lakes)
  #Last dates that appear in the data sets
  last_date = vector("list",n_lakes)
  last_rain = vector("list",n_lakes)
  #Load the current data files
  current_date = ymd( Sys.Date() ) #Current date. Could be set to other
  lake_table[[1]] = read.csv(file = "./../data/men_hist.csv")
  lake_table[[1]][,"time"] = ymd(lake_table[[1]][,"time"])
  lake_table[[2]] = read.csv(file = "./../data/mon_hist.csv")
  lake_table[[2]][,"time"] = ymd(lake_table[[2]][,"time"])
  daily_precip[[1]] = read.csv(file = "./../data/rain_hist.csv")
  daily_precip[[1]][,"time"] = ymd(daily_precip[[1]][,"time"])
  daily_precip[[2]] = daily_precip[[1]]
  #Make backups of previous file:
  file.copy(from = "./../data/men_hist.csv", to ="./../data/men_hist.csv.bck")
  file.copy(from = "./../data/mon_hist.csv", to ="./../data/mon_hist.csv.bck")
  file.copy(from = "./../data/rain_hist.csv", to ="./../data/rain_hist.csv.bck")
  #Get the last dates entered. If they don't match to the current date
  #then update the data with the functions updateLake and updateRain. 
  #Write the new files.  
  for ( n in 1:n_lakes){ 
    #For the lakes
    last_date[[n]] = lake_table[[n]][nrow(lake_table[[n]]),"time"]
    if(last_date[[n]] != current_date  ){ 
      lake_table[[n]] = rbind(lake_table[[n]],
          updateLake(lake_table[[n]], start_date = last_date[[n]], 
          current_date = current_date ) )
      lake_table[[n]] = lake_table[[n]][-( (nrow(lake_table[[n]])-2):
        nrow(lake_table[[n]])), ]
    }
    #For the precip
    last_rain[[n]] = daily_precip[[n]][nrow(daily_precip[[n]]),"time"]
    if(last_rain[[n]] != current_date  ){ 
      daily_precip[[n]] = rbind(daily_precip[[n]],
          updateRain(daily_precip[[n]], start_date = last_rain[[n]], 
            current_date = current_date) )
      daily_precip[[n]] = daily_precip[[n]][-( (nrow(daily_precip[[n]])-1):
        nrow(daily_precip[[n]])), ]
    }
    lake_data[[n]] = lake_table[[n]] %>%
        inner_join(daily_precip[[n]], by = "time" ) 
  }
#This is a helper function for lake level. If the data is not up to date, do
#all of the necessary data procesing. 
updateLake = function (lake_table, start_date, current_date) {
  #Use the USGS address format to get data over the date range
  url1 = paste(url_base[1], site_keys[n], "&startDT=", start_date,
    "&endDT=",current_date,"&parameterCd=00060,00065&siteStatus=all",sep="" )
  lt_temp = as.data.frame(read_delim(print(url1), comment = "#", delim="\t"))
  lt_temp = lt_temp[-1,]
  lt_temp[,"datetime"] = as.POSIXct(lt_temp[,"datetime"],
                       format = '%Y-%m-%d %H:%M')
  lt_temp[,5] = as.numeric(as.character(lt_temp[,5]))
  lt_temp = lt_temp[,c(3, 5)]
  colnames(lt_temp ) = c("time", "level")
  #Data seem to be at 15 min intervals by default. Aggregate these into 
  #a mean daily level
  lt_temp = lt_temp %>%
      mutate(time = as.Date(ymd_hms(time))) %>%
      group_by(time) %>%
      summarise(level = mean(level, na.rm = TRUE)) %>%
      as.data.frame()
  return(lt_temp)
}
#This is a helper function for rain. If the data is not up to date, do
#all of the necessary data procesing. 
updateRain = function (daily_rain, start_date, current_date) {
  #Get the matching precipitation data from the NASA POWER collection
  rn_temp = get_power(
    community = "ag",
    lonlat = c(43.0930, -89.3727),
    pars =  nasa_pars,
    dates = c(paste(start_date), paste(current_date)),
    temporal_api = "daily"
  ) %>% as.data.frame()
  rn_temp = rn_temp[,c(7,8)]
  colnames(rn_temp) = c("time", "rn")
  return(rn_temp)
}
  #Preallocate the important historical data tables 
  lake_table = vector("list", n_lakes)
  daily_precip = vector("list", n_lakes)
  #Last dates that appear in the data sets
  last_date = vector("list",n_lakes)
  last_rain = vector("list",n_lakes)
  #Load the current data files
  current_date = ymd( Sys.Date() ) #Current date. Could be set to other
  lake_table[[1]] = read.csv(file = "./../data/men_hist.csv")
  lake_table[[1]][,"time"] = ymd(lake_table[[1]][,"time"])
  lake_table[[2]] = read.csv(file = "./../data/mon_hist.csv")
  lake_table[[2]][,"time"] = ymd(lake_table[[2]][,"time"])
  daily_precip[[1]] = read.csv(file = "./../data/rain_hist.csv")
  daily_precip[[1]][,"time"] = ymd(daily_precip[[1]][,"time"])
  daily_precip[[2]] = daily_precip[[1]]
  #Make backups of previous file:
  file.copy(from = "./../data/men_hist.csv", to ="./../data/men_hist.csv.bck")
  file.copy(from = "./../data/mon_hist.csv", to ="./../data/mon_hist.csv.bck")
  file.copy(from = "./../data/rain_hist.csv", to ="./../data/rain_hist.csv.bck")
  #Get the last dates entered. If they don't match to the current date
  #then update the data with the functions updateLake and updateRain. 
  #Write the new files.  
  for ( n in 1:n_lakes){ 
    #For the lakes
    last_date[[n]] = lake_table[[n]][nrow(lake_table[[n]]),"time"]
    if(last_date[[n]] != current_date  ){ 
      lake_table[[n]] = rbind(lake_table[[n]],
          updateLake(lake_table[[n]], start_date = last_date[[n]], 
          current_date = current_date ) )
      lake_table[[n]] = lake_table[[n]][-( (nrow(lake_table[[n]])-2):
        nrow(lake_table[[n]])), ]
    }
    #For the precip
    last_rain[[n]] = daily_precip[[n]][nrow(daily_precip[[n]]),"time"]
    if(last_rain[[n]] != current_date  ){ 
      daily_precip[[n]] = rbind(daily_precip[[n]],
          updateRain(daily_precip[[n]], start_date = last_rain[[n]], 
            current_date = current_date) )
      daily_precip[[n]] = daily_precip[[n]][-( (nrow(daily_precip[[n]])-1):
        nrow(daily_precip[[n]])), ]
    }
    lake_data[[n]] = lake_table[[n]] %>%
        inner_join(daily_precip[[n]], by = "time" ) 
  }
dim(lake_data[[1]])
dim(lake_data[[2]])
tail(lake_data[[1]])
tail(lake_data[[2]])
tail(lake_data[[1]])
 write.table(lake_data[[1]][,1:2], file = "./../data/men_hist.csv", sep=",")
  write.table(lake_data[[2]][,1:2], file = "./../data/mon_hist.csv", sep=",")
  write.table(lake_data[[1]][,c(1,3)], file = "./../data/rain_hist.csv", sep=",")
#Preallocate the important historical data tables 
  lake_table = vector("list", n_lakes)
  daily_precip = vector("list", n_lakes)
  #Final data set
  lake_data = vector("list", n_lakes)
  #Load the historic data sets
  lake_table[[1]] = read.csv(file = "./../data/men_hist.csv")
  lake_table[[1]][,"time"] = ymd(lake_table[[1]][,"time"])
  lake_table[[2]] = read.csv(file = "./../data/mon_hist.csv")
  lake_table[[2]][,"time"] = ymd(lake_table[[2]][,"time"])
  daily_precip[[1]] = read.csv(file = "./../data/rain_hist.csv")
  daily_precip[[1]][,"time"] = ymd(daily_precip[[1]][,"time"])
  daily_precip[[2]] = daily_precip[[1]]
  #Final processing steps of the raw data which joins lake
  #and precip and truncates to desired start date. 
  for (n in 1: n_lakes){
    #Join the lake and rain data to match up dates
    lake_data[[n]] = lake_table[[n]] %>%
          inner_join(daily_precip[[n]], by = "time" )
    #Truncate the data set so that we only have from real_start
    #onwards. 
    lake_data[[n]] = lake_data[[n]][lake_data[[n]][,"time"] 
                      >= real_start[[n]], ]
    #Do some processing to remove ice-on days (approximately). This 
    #function automatically removes winter days and converts data 
    #table to a timeseries (ts) object 
    lake.tmp = remove.days(lake_data[[n]]$level, year(real_start[[n]] ) )
    colnames(lake.tmp) = "level"
    rn.tmp = remove.days(lake_data[[n]]$rn, year(real_start[[n]] ) )
    colnames(rn.tmp) = "rn"
    #This final step creates the full data object, with lags of 
    #lake level for autocorrelation and lags of rain for delayed
    #rain input. 
    lake_data[[n]] = make.flashiness.object(lake.tmp, rn.tmp, lags)
  }
tail(lake_data[[1]])
tail(lake_data[[2]])
    n_lakes = length(model_form)
    #Store fitted models
    lake_models = vector("list", n_lakes)
    #Loop over lakes and fit models. Assuming that best-fit models have 
    #already been determined by AIC and GCV. 
    for(n in 1:n_lakes){ 
        # Use the residuals from the GARCH model so that the trends in variance are
        # removed. Note, this version only fits the GARCH part because the AR will be
        # fit by the GAM: 
        lake_gfit1=garchFit( ~arma(0,0)+garch(1,1),
                     data=na.exclude(lake_data[[n]][,2,drop=F]), trace=F)
        # New lake-level time series based on residuals
        lake_new=as.matrix(lake_gfit1@residuals)
        # New time series after removing NAs in the rain
        rn_new=as.matrix(lake_data[[n]]$rn[!is.na(lake_data[[n]][,"rn",drop=T])])
        lake_new = as.matrix(lake_new[!is.na(lake_data[[n]][,"rn",drop=T])])
        colnames(rn_new) = "rn"
        colnames(lake_new) = "level"
        #Combine all of the data, add the lagged data, and turn into ts
        lake_r = make.flashiness.object( lake_new , rn_new, lags)
        # The best-fit GAMs were determined in Usinowicz et al. 2016. 
        # Those are what are fit here.
        # Use bam() (instead of gam()) from mgcv because it is designed for 
        # large data sets.
        lake_models[[n]] = bam ( as.formula((model_form [[n]] )), data=lake_r)
    }
head(lake_data[[1]])
head(lake_data[[2]])
head(lake_data[[1]])
model_form [[1]] = "level ~ 
    s(level1,bs=\"cr\",k=6)+s(level2,bs=\"cr\",k=6)+
    s(rn1,bs=\"cr\",k=6)+
    s(rn2,bs=\"cr\",k=6)+s(rn3,bs=\"cr\",k=6)+
    te(rn,time,k=20)+te(rn1,time,k=20)+te(rn2,time,k=20)+
    te(rn3,time,k=20)"
model_form [[2]] = "level ~ 
    s(level1,bs=\"cr\",k=6)+s(level2,bs=\"cr\",k=6)+s(level3,bs=\"cr\",k=6)+
    s(rn1,bs=\"cr\",k=6)+
    s(rn2,bs=\"cr\",k=6)+s(rn3,bs=\"cr\",k=6)+
    te(rn,time,k=20)+te(rn1,time,k=20)+te(rn2,time,k=20)+
    te(rn3,time,k=20)"
    n_lakes = length(model_form)
    #Store fitted models
    lake_models = vector("list", n_lakes)
    #Loop over lakes and fit models. Assuming that best-fit models have 
    #already been determined by AIC and GCV. 
    for(n in 1:n_lakes){ 
        # Use the residuals from the GARCH model so that the trends in variance are
        # removed. Note, this version only fits the GARCH part because the AR will be
        # fit by the GAM: 
        lake_gfit1=garchFit( ~arma(0,0)+garch(1,1),
                     data=na.exclude(lake_data[[n]][,2,drop=F]), trace=F)
        # New lake-level time series based on residuals
        lake_new=as.matrix(lake_gfit1@residuals)
        # New time series after removing NAs in the rain
        rn_new=as.matrix(lake_data[[n]]$rn[!is.na(lake_data[[n]][,"rn",drop=T])])
        lake_new = as.matrix(lake_new[!is.na(lake_data[[n]][,"rn",drop=T])])
        colnames(rn_new) = "rn"
        colnames(lake_new) = "level"
        #Combine all of the data, add the lagged data, and turn into ts
        lake_r = make.flashiness.object( lake_new , rn_new, lags)
        # The best-fit GAMs were determined in Usinowicz et al. 2016. 
        # Those are what are fit here.
        # Use bam() (instead of gam()) from mgcv because it is designed for 
        # large data sets.
        lake_models[[n]] = bam ( as.formula((model_form [[n]] )), data=lake_r)
    }
lake_models2 = lake_models
AIC(lake_models2[[1]])
AIC(lake_models2[[2]])
model_form[[1]]
model_form [[1]] = "level ~ s(time, bs = \"cr\", k = 100)+
    s(level1,bs=\"cr\",k=6)+s(level2,bs=\"cr\",k=6)+
    s(rn1,bs=\"cr\",k=6)+
    s(rn2,bs=\"cr\",k=6)+s(rn3,bs=\"cr\",k=6)+
    te(rn,time,k=20)+te(rn1,time,k=20)+te(rn2,time,k=20)+
    te(rn3,time,k=20)"
model_form [[2]] = "level ~ s(time, bs = \"cr\", k = 100)+
    s(level1,bs=\"cr\",k=6)+s(level2,bs=\"cr\",k=6)+s(level3,bs=\"cr\",k=6)+
    s(rn1,bs=\"cr\",k=6)+
    s(rn2,bs=\"cr\",k=6)+s(rn3,bs=\"cr\",k=6)+
    te(rn,time,k=20)+te(rn1,time,k=20)+te(rn2,time,k=20)+
    te(rn3,time,k=20)"
    n_lakes = length(model_form)
    #Store fitted models
    lake_models = vector("list", n_lakes)
    #Loop over lakes and fit models. Assuming that best-fit models have 
    #already been determined by AIC and GCV. 
    for(n in 1:n_lakes){ 
        # Use the residuals from the GARCH model so that the trends in variance are
        # removed. Note, this version only fits the GARCH part because the AR will be
        # fit by the GAM: 
        lake_gfit1=garchFit( ~arma(0,0)+garch(1,1),
                     data=na.exclude(lake_data[[n]][,2,drop=F]), trace=F)
        # New lake-level time series based on residuals
        lake_new=as.matrix(lake_gfit1@residuals)
        # New time series after removing NAs in the rain
        rn_new=as.matrix(lake_data[[n]]$rn[!is.na(lake_data[[n]][,"rn",drop=T])])
        lake_new = as.matrix(lake_new[!is.na(lake_data[[n]][,"rn",drop=T])])
        colnames(rn_new) = "rn"
        colnames(lake_new) = "level"
        #Combine all of the data, add the lagged data, and turn into ts
        lake_r = make.flashiness.object( lake_new , rn_new, lags)
        # The best-fit GAMs were determined in Usinowicz et al. 2016. 
        # Those are what are fit here.
        # Use bam() (instead of gam()) from mgcv because it is designed for 
        # large data sets.
        lake_models[[n]] = bam ( as.formula((model_form [[n]] )), data=lake_r)
    }
AIC(lake_models[[1]])
AIC(lake_models[[2]])
lake_models2=lake_models
model_form [[1]] = "level ~ 
    s(level1,bs=\"cr\",k=6)+s(level2,bs=\"cr\",k=6)+s(level3,bs=\"cr\",k=6)+
    s(rn,bs=\"cr\",k=6)+s(rn1,bs=\"cr\",k=6)+
    s(rn2,bs=\"cr\",k=6)+s(rn3,bs=\"cr\",k=6)+
    te(rn,rn1,k=20)+te(rn1,rn2,k=20)+te(rn2,rn3,k=20)"
model_form [[2]] = "level ~ 
    s(level1,bs=\"cr\",k=6)+s(level2,bs=\"cr\",k=6)+s(level3,bs=\"cr\",k=6)+
    s(rn,bs=\"cr\",k=6)+s(rn1,bs=\"cr\",k=6)+
    s(rn2,bs=\"cr\",k=6)+s(rn3,bs=\"cr\",k=6)+
    te(rn,rn1,k=20)+te(rn1,rn2,k=20)+te(rn2,rn3,k=20)"
model_form [[1]] = "level ~ 
    s(level1,bs=\"cr\",k=6)+s(level2,bs=\"cr\",k=6)+
    s(rn,bs=\"cr\",k=6)+s(rn1,bs=\"cr\",k=6)+
    s(rn2,bs=\"cr\",k=6)+s(rn3,bs=\"cr\",k=6)+
    te(rn,rn1,k=20)+te(rn1,rn2,k=20)+te(rn2,rn3,k=20)"
model_form [[2]] = "level ~ 
    s(level1,bs=\"cr\",k=6)+s(level2,bs=\"cr\",k=6)+s(level3,bs=\"cr\",k=6)+
    s(rn,bs=\"cr\",k=6)+s(rn1,bs=\"cr\",k=6)+
    s(rn2,bs=\"cr\",k=6)+s(rn3,bs=\"cr\",k=6)+
    te(rn,rn1,k=20)+te(rn1,rn2,k=20)+te(rn2,rn3,k=20)"
    n_lakes = length(model_form)
    #Store fitted models
    lake_models = vector("list", n_lakes)
    #Loop over lakes and fit models. Assuming that best-fit models have 
    #already been determined by AIC and GCV. 
    for(n in 1:n_lakes){ 
        # Use the residuals from the GARCH model so that the trends in variance are
        # removed. Note, this version only fits the GARCH part because the AR will be
        # fit by the GAM: 
        lake_gfit1=garchFit( ~arma(0,0)+garch(1,1),
                     data=na.exclude(lake_data[[n]][,2,drop=F]), trace=F)
        # New lake-level time series based on residuals
        lake_new=as.matrix(lake_gfit1@residuals)
        # New time series after removing NAs in the rain
        rn_new=as.matrix(lake_data[[n]]$rn[!is.na(lake_data[[n]][,"rn",drop=T])])
        lake_new = as.matrix(lake_new[!is.na(lake_data[[n]][,"rn",drop=T])])
        colnames(rn_new) = "rn"
        colnames(lake_new) = "level"
        #Combine all of the data, add the lagged data, and turn into ts
        lake_r = make.flashiness.object( lake_new , rn_new, lags)
        # The best-fit GAMs were determined in Usinowicz et al. 2016. 
        # Those are what are fit here.
        # Use bam() (instead of gam()) from mgcv because it is designed for 
        # large data sets.
        lake_models[[n]] = bam ( as.formula((model_form [[n]] )), data=lake_r)
    }
AIC(lake_models[[1]],lake_models2[[1]])
AIC(lake_models[[2]],lake_models2[[2]])
    save(file = "lakeGAMsLp_full.var", lake_models ) #Too big :(
q()
