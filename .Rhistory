##############################################################
#load libraries
##############################################################
library(tidyverse)
library(lubridate)
library(rvest) #To parse table from webpage
library(nasapower) #API for NASA data, for precipitation
library(GGally)
source("./functions/flash_functions.R")
#For tensorflow:
library(keras)
library(tidymodels)
library(recipes)
#Note:ignore the instructions to load "tensorflow" directly
#Install should happen via: 
#    install.packages("keras")
#    library(keras)
#    install_keras()
##############################################################
#Key user-defined variables
##############################################################
current_date = Sys.Date() #Current date. Could be set to other
#How long of a data set? Currently 30 years
yl1 = 30
start_date = current_date  -  years(yl1)
#USGS site keys. Currently includes Mendota and Monona
site_keys = c("05428000", "05429000")
#The base urls used by the USGS for lake data
url_base = c("https://waterservices.usgs.gov/nwis/iv/?format=rdb&sites=")
#Parameters from the NASA POWER data collection
nasa_pars = c("PRECTOTCORR")
#Lags in rain and lake-level data
lags = 10
#The days that we count as winter to exclude ice-on days
winter=c(334,120)
##############################################################
#PART 1: Data processing
##############################################################
#Import lake-level time series as the response. These data for
#Lake Stage Level are from the USGS data base, e.g. for Lake
#Mendota:  
#https://waterservices.usgs.gov/nwis/iv/?format=rdb&sites=05427718&startDT=2013-09-21&endDT=2014-09-21&parameterCd=00045&siteStatus=all
#Preallocate and get number of lakes
n_lakes = length(site_keys)
lake_table = vector("list", n_lakes)
daily_precip = vector("list", n_lakes)
real_start = vector("list", n_lakes)
#Final data set
lake_data = vector("list", n_lakes)
#Loop over lakes to get stage level and dates: 
for(n in 1:n_lakes){ 
     #Use the USGS address format to get data over the date range
     url1 = paste(url_base[1], site_keys[n], "&startDT=", start_date,
          "&endDT=",current_date,"&parameterCd=00060,00065&siteStatus=all",sep="" )
     lake_table[[n]]= as.data.frame(read_delim(print(url1), comment = "#", delim="\t"))
     lake_table[[n]] = lake_table[[n]][-1,]
     lake_table[[n]][,"datetime"] = as.POSIXct(lake_table[[n]][,"datetime"],
                       format = '%Y-%m-%d %H:%M')
     lake_table[[n]][,5] = as.numeric(as.character(lake_table[[n]][,5]))
     lake_table[[n]] = lake_table[[n]][,c(3, 5)]
     colnames(lake_table[[n]] ) = c("datetime", "level")
     #Data seem to be at 15 min intervals by default. Aggregate these into 
     #a mean daily level
     lake_table[[n]] = lake_table[[n]] %>%
          mutate(day = as.Date(ymd_hms(datetime))) %>%
          group_by(day) %>%
          summarise(level = mean(level, na.rm = TRUE)) %>%
          as.data.frame()
     #Just in case we requested back too far, what is the actual start 
     #date of the retrieved data?
     real_start[[n]] = lake_table[[n]][1,"day"]
     #Get the matching precipitation data from the NASA POWER collection
     daily_precip[[n]] = get_power(
       community = "ag",
       lonlat = c(43.0930, -89.3727),
       pars =  nasa_pars,
       dates = c(paste(real_start[[n]]), paste(current_date)),
       temporal_api = "daily"
     ) %>% as.data.frame()
     daily_precip[[n]] = daily_precip[[n]][,c(7,8)]
     colnames(daily_precip[[n]]) = c("day", "rn")
     #Join the lake and rain data to match up dates
     lake_table[[n]] = lake_table[[n]] %>%
               inner_join(daily_precip[[n]], by = "day" ) 
     #Do some processing to remove ice-on days (approximately). This 
     #function automatically removes winter days and converts data 
     #table to a timeseries (ts) object 
     lake.tmp = remove.days(lake_table[[n]]$level, year(real_start[[n]] ) )
     colnames(lake.tmp) = "level"
     rn.tmp = remove.days(lake_table[[n]]$rn, year(real_start[[n]] ) )
     colnames(rn.tmp) = "rn"
     #This final step creates the full data object, with lags of 
     #lake level for autocorrelation and lags of rain for delayed
     #rain input. 
     lake_data[[n]] = make.flashiness.object(lake.tmp, rn.tmp, lags)
}
##############################################################
#PART 2: tensorflow
##############################################################
#Store fitted models
lake_models = vector("list", n_lakes)
#Performance and prediction 
pred_train = vector("list", n_lakes)
pred_test = vector("list", n_lakes)
#This function is for tensorflow to create a DNN. This is 
#analogous to the "model form"  
build_and_compile_model = function(norm) {
  model = keras_model_sequential() %>%
    norm() %>%
    layer_dense(64, activation = 'relu') %>%
    layer_dense(64, activation = 'relu') %>%
    layer_dense(1)
  model %>% compile(
    loss = 'mean_absolute_error',
    optimizer = optimizer_adam(0.001)
  )
  model
}
q()
